{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassineElghazel/video-llava-prompt-study/blob/main/Extension/gemini_1_5pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgcmgtdE8NX"
      },
      "source": [
        "##General Setup\n",
        "\n",
        "Imports and persistant storage in google drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIRFqbWM2x5T",
        "outputId": "dc49ad1a-65f8-4c6a-b5c9-a2a327acb67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71f5JJI5YUaH"
      },
      "outputs": [],
      "source": [
        "# --- standard library ---\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "\n",
        "# --- third-party ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import av\n",
        "import torch\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from google.api_core.exceptions import ResourceExhausted, BadRequest, InternalServerError\n",
        "\n",
        "# --- Colab-specific ---\n",
        "from google.colab import drive, userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM6xE9aAE2AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7386870-0b09-4089-f7ad-0da86012e3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F5gnv8WFCzn"
      },
      "outputs": [],
      "source": [
        "CUT_VIDEO_OUT = \"/content/drive/MyDrive/model_2.0/model/cut_videos_6tol\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "collapsed": true,
        "id": "T-sMg9_4THT0",
        "outputId": "c4a81ecc-cdab-40e8-a179-3703579bea17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   idx                        annotation_uid  \\\n",
              "0    0  93891b44-c00f-43d9-885b-92fdce39128c   \n",
              "1    1  00bb3571-8b35-40ba-8163-a1a0fd20b886   \n",
              "2    2  d8fb0c3d-39b1-4bb2-b800-fc56f12ab120   \n",
              "3    3  e8b1352a-6e0c-4f86-8a52-afe93743abb6   \n",
              "4    4  2bc5aa4c-3114-497e-9d13-e229277fe11b   \n",
              "\n",
              "                               clip_uid  \\\n",
              "0  75d3fc52-3776-47d4-b7fd-8074d30b06d1   \n",
              "1  60e7e14d-cbed-46d1-924d-6ce451ea7d7c   \n",
              "2  e1c79556-e8af-4e26-bc4c-633100277239   \n",
              "3  efc190a8-45de-4ce5-b480-b722403bcec1   \n",
              "4  cc6270fd-3c0d-4dda-bcb4-52cefc0224d7   \n",
              "\n",
              "                                        sentence predicted_times  \\\n",
              "0                Where did I put the chopsticks.     (0.0, 3.75)   \n",
              "1                       What cable did I remove?  (78.75, 86.25)   \n",
              "2                  Did I close the refrigerator?     (0.0, 3.75)   \n",
              "3  Where was the scissors before I picked it up?   (93.75, 97.5)   \n",
              "4          In what location did I open the door?     (3.75, 7.5)   \n",
              "\n",
              "            exact_times                             video_uid  \\\n",
              "0        (3.319, 4.619)  413fe086-1745-4573-b75b-e7d26ff72df9   \n",
              "1  (84.32902, 84.71152)  03e90bbc-7d6b-423c-84d9-b5be3eff11c5   \n",
              "2    (0.65536, 2.00097)  4ce119de-0f42-4bd1-b387-9e19643fdddc   \n",
              "3        (87.0, 89.306)  ff6d3d52-dda5-46dd-8515-b9b772933030   \n",
              "4       (6.2829, 8.428)  432cb803-6be5-47bc-8443-6bb5b9051667   \n",
              "\n",
              "        ground truth  \n",
              "0   beside the stove  \n",
              "1          usb drive  \n",
              "2                yes  \n",
              "3  on the plate rack  \n",
              "4    in the bathroom  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f49599b-923a-4701-9362-2ef9bde22feb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>annotation_uid</th>\n",
              "      <th>clip_uid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>predicted_times</th>\n",
              "      <th>exact_times</th>\n",
              "      <th>video_uid</th>\n",
              "      <th>ground truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>93891b44-c00f-43d9-885b-92fdce39128c</td>\n",
              "      <td>75d3fc52-3776-47d4-b7fd-8074d30b06d1</td>\n",
              "      <td>Where did I put the chopsticks.</td>\n",
              "      <td>(0.0, 3.75)</td>\n",
              "      <td>(3.319, 4.619)</td>\n",
              "      <td>413fe086-1745-4573-b75b-e7d26ff72df9</td>\n",
              "      <td>beside the stove</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>00bb3571-8b35-40ba-8163-a1a0fd20b886</td>\n",
              "      <td>60e7e14d-cbed-46d1-924d-6ce451ea7d7c</td>\n",
              "      <td>What cable did I remove?</td>\n",
              "      <td>(78.75, 86.25)</td>\n",
              "      <td>(84.32902, 84.71152)</td>\n",
              "      <td>03e90bbc-7d6b-423c-84d9-b5be3eff11c5</td>\n",
              "      <td>usb drive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>d8fb0c3d-39b1-4bb2-b800-fc56f12ab120</td>\n",
              "      <td>e1c79556-e8af-4e26-bc4c-633100277239</td>\n",
              "      <td>Did I close the refrigerator?</td>\n",
              "      <td>(0.0, 3.75)</td>\n",
              "      <td>(0.65536, 2.00097)</td>\n",
              "      <td>4ce119de-0f42-4bd1-b387-9e19643fdddc</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>e8b1352a-6e0c-4f86-8a52-afe93743abb6</td>\n",
              "      <td>efc190a8-45de-4ce5-b480-b722403bcec1</td>\n",
              "      <td>Where was the scissors before I picked it up?</td>\n",
              "      <td>(93.75, 97.5)</td>\n",
              "      <td>(87.0, 89.306)</td>\n",
              "      <td>ff6d3d52-dda5-46dd-8515-b9b772933030</td>\n",
              "      <td>on the plate rack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2bc5aa4c-3114-497e-9d13-e229277fe11b</td>\n",
              "      <td>cc6270fd-3c0d-4dda-bcb4-52cefc0224d7</td>\n",
              "      <td>In what location did I open the door?</td>\n",
              "      <td>(3.75, 7.5)</td>\n",
              "      <td>(6.2829, 8.428)</td>\n",
              "      <td>432cb803-6be5-47bc-8443-6bb5b9051667</td>\n",
              "      <td>in the bathroom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f49599b-923a-4701-9362-2ef9bde22feb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f49599b-923a-4701-9362-2ef9bde22feb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f49599b-923a-4701-9362-2ef9bde22feb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ee678805-acf8-407c-b86d-7a272a8f7149\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee678805-acf8-407c-b86d-7a272a8f7149')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ee678805-acf8-407c-b86d-7a272a8f7149 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations_df",
              "summary": "{\n  \"name\": \"annotations_df\",\n  \"rows\": 197,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 60,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 157,\n        \"samples\": [\n          172,\n          60,\n          181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotation_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 197,\n        \"samples\": [\n          \"b4126ae8-0788-443e-969e-d83dcadd9e56\",\n          \"a6cd5d8f-1de0-4c51-bc5a-1d2c2d7fb086\",\n          \"cac0825d-a367-4213-a934-ded237c49639\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clip_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"9c82fb4e-b385-46a8-b829-45723b17cdba\",\n          \"2c2bda8d-69a3-4a90-9ad6-f6715bc99f39\",\n          \"c96cc4a2-0e82-4c79-b34f-c5b853cd3ab2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 195,\n        \"samples\": [\n          \"where was the ceramic plate before I picked it?\",\n          \"Where was the nylon paper before I picked it?\",\n          \"Where is the dish brush?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_times\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 112,\n        \"samples\": [\n          \"(0.0, 9.375)\",\n          \"(427.0249938964844, 434.51666259765625)\",\n          \"(270.0, 273.75)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact_times\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 194,\n        \"samples\": [\n          \"(53.0, 54.0)\",\n          \"(7.0, 8.0)\",\n          \"(26.29648, 32.507)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 149,\n        \"samples\": [\n          \"28dca1ee-7675-47a2-be89-3fa5979789e1\",\n          \"eb04561c-2ffd-4ea1-aab4-7cadc24db9f9\",\n          \"a7062d19-5af7-42fe-ad8c-2d7442ddbe48\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 172,\n        \"samples\": [\n          \"outside a store\",\n          \"In the baker's hand\",\n          \"Power cable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "annotations_path = f'/content/drive/MyDrive/model_2.0/model/GroundTruth.xlsx'\n",
        "annotations_df = pd.read_excel(annotations_path)\n",
        "\n",
        "annotations_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2uwut71DyCj"
      },
      "source": [
        "Define model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Set and configure API key\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")"
      ],
      "metadata": {
        "id": "zAxmw1gCWXpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpWnePWWD-FI"
      },
      "source": [
        "Define model usage functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def build_general_prompt(sentence: str) -> str:\n",
        "    q = sentence.strip().lower()\n",
        "\n",
        "    prompt = (\n",
        "        \"### SYSTEM:\\n\"\n",
        "        \"You are a helpful assistant that answers questions based on visual summaries of video segments.\\n\"\n",
        "        \"Use only the information visible in the video.\\n\"\n",
        "        \"Be accurate and respond in **5 words or fewer**.\\n\\n\"\n",
        "\n",
        "        \"### CONTEXT:\\n\"\n",
        "        \"<video>\\n\"\n",
        "        \"(Visual summary of the video is shown here)\\n\\n\"\n",
        "\n",
        "        f\"### USER:\\nQuestion: {sentence.strip()}\\n\"\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # üîç Task Classification Logic\n",
        "    # -------------------------------\n",
        "\n",
        "    # 1. Counting tasks\n",
        "    if re.search(r'\\bhow many\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"This is a counting task. Count only the visible items.\\n\"\n",
        "            \"Return the number.\\n\"\n",
        "        )\n",
        "\n",
        "    # 2. Color detection\n",
        "    elif re.search(r'\\bwhat (color|colour)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the visible color of the mentioned object.\\n\"\n",
        "        )\n",
        "\n",
        "    # 3. Location questions (debiased)\n",
        "    elif re.search(r'\\bwhere (is|was|did|do)\\b|\\bin what location\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the object's visible spatial location based only on the video summary.\\n\"\n",
        "            \"Use only location descriptions directly inferable from the visual context.\\n\"\n",
        "        )\n",
        "\n",
        "    # 4. Action-object identification\n",
        "    elif re.search(r'\\bwhat\\b', q) and re.search(\n",
        "        r'\\b(remove|pick|grab|cut|wash|tie|wipe|press|use|carry|put|insert|drop|place)\\b', q\n",
        "    ):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the object involved in the action based on what is clearly visible in the video.\\n\"\n",
        "        )\n",
        "\n",
        "    # 5. Tool/machine recognition\n",
        "    elif re.search(r'\\bwhat\\b', q) and re.search(r'\\b(tool|machine|device|object|item|equipment)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Determine what object, tool, or machine was used. Base your answer only on visible cues.\\n\"\n",
        "        )\n",
        "\n",
        "    # 6. Human interaction\n",
        "    elif re.search(r'\\bwho did (i|he|she|we|they) (talk|interact)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Name the person involved in the interaction.\\n\"\n",
        "        )\n",
        "\n",
        "    # 7. Yes/no ‚Äî fallback only\n",
        "    elif (\n",
        "        re.search(r'\\bdid\\b.*\\b(i|we|they|he|she)\\b', q)\n",
        "        and not re.search(r'\\bwhat\\b', q)\n",
        "        and not re.search(r'\\bwho\\b', q)\n",
        "    ):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"This is a yes/no question. Respond with 'yes' or 'no' based on visible evidence only.\\n\"\n",
        "        )\n",
        "\n",
        "    # 8. Fallback\n",
        "    else:\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Answer briefly and accurately using only the information visible in the video summary.\\n\"\n",
        "        )\n",
        "\n",
        "    prompt += \"\\nAnswer:\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "X2JKgzdBbYLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_grid_image(frames, grid_size=(2, 4)):\n",
        "    assert len(frames) == 8, \"Exactly 8 frames are expected\"\n",
        "    frame_width, frame_height = frames[0].size\n",
        "    grid_width = grid_size[1] * frame_width\n",
        "    grid_height = grid_size[0] * frame_height\n",
        "\n",
        "    grid_image = Image.new('RGB', (grid_width, grid_height))\n",
        "\n",
        "    for idx, frame in enumerate(frames):\n",
        "        row = idx // grid_size[1]\n",
        "        col = idx % grid_size[1]\n",
        "        grid_image.paste(frame, (col * frame_width, row * frame_height))\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "\n",
        "def read_video_frames(container, indices):\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > indices[-1]:\n",
        "            break\n",
        "        if i in indices:\n",
        "            frames.append(frame.to_ndarray(format='rgb24'))\n",
        "\n",
        "    while len(frames) < 8:\n",
        "        frames.append(frames[-1])\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def frame_to_pil(frame):\n",
        "    return Image.fromarray(frame)\n",
        "\n",
        "\n",
        "def prepare_gemini_inputs(row, video_segments_dir):\n",
        "    clip_uid = row['clip_uid']\n",
        "    annotation_uid = row['annotation_uid']\n",
        "    sentence = row['sentence']\n",
        "    idx = int(row['idx'])\n",
        "    video_segment_path = os.path.join(video_segments_dir, f\"{idx}_{clip_uid}_{annotation_uid}.mp4\")\n",
        "\n",
        "    if not os.path.exists(video_segment_path):\n",
        "        print(f\"Video file {video_segment_path} not found.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        container = av.open(video_segment_path)\n",
        "        if not container.streams.video:\n",
        "            print(f\"No video stream in {video_segment_path}.\")\n",
        "            return None, None\n",
        "\n",
        "        total_frames = container.streams.video[0].frames\n",
        "        indices = np.linspace(0, total_frames - 1, num=8, dtype=int)\n",
        "        frames = read_video_frames(container, indices)\n",
        "        pil_frames = [frame_to_pil(f) for f in frames]\n",
        "        grid_image = make_grid_image(pil_frames)  # ‚úÖ create one image from 8\n",
        "\n",
        "        prompt = build_general_prompt(sentence)\n",
        "        return [grid_image], prompt  # ‚úÖ Return one image inside a list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_segment_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def process_row_with_gemini(row):\n",
        "    images, prompt = prepare_gemini_inputs(row, CUT_VIDEO_OUT)\n",
        "    if images is None or not all(isinstance(img, Image.Image) for img in images):\n",
        "        print(f\"‚ö†Ô∏è Skipping idx {row['idx']} due to invalid image(s)\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            [prompt] + images,\n",
        "            generation_config={\"max_output_tokens\": 3000}\n",
        "        )\n",
        "\n",
        "        if not response.parts:\n",
        "          print(f\"‚ö†Ô∏è No content returned for idx {row['idx']}. Prompt:\\n{prompt}\\n\")\n",
        "          return None\n",
        "\n",
        "        answer = response.text.strip()\n",
        "        print(answer)\n",
        "        times = row['predicted_times'].strip('()').split(\",\")\n",
        "\n",
        "        return {\n",
        "          'idx' : row['idx'],\n",
        "            'annotation_uid' : row['annotation_uid'],\n",
        "            'clip_uid' : row['clip_uid'],\n",
        "            'sentence' : row['sentence'],\n",
        "            'predicted_times' : row['predicted_times'],\n",
        "            'exact_times' : row['exact_times'],\n",
        "            'video_uid' : row['video_uid'],\n",
        "            'ground truth' : row['ground truth'],\n",
        "            'answer': answer.split(\"Answer:\")[-1].strip() if \"Answer:\" in answer else answer.strip(),\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating response for idx {row['idx']}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "aGdlIevR91mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEx-NLPgEH4E"
      },
      "source": [
        "Feed prompts and video into model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/model_2.0/model/gemini_video_qa_ans_1.5_pro_3000.csv\"\n",
        "CHECKPOINT_EVERY = 20\n",
        "\n",
        "answers = []\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    answers_df = pd.read_csv(CHECKPOINT_PATH)\n",
        "    processed_keys = set(answers_df.apply(lambda row: f\"{row['clip_uid']}::{row['annotation_uid']}\", axis=1))\n",
        "    answers = answers_df.to_dict(orient=\"records\")\n",
        "    print(f\"üîÅ Resuming from checkpoint: {len(answers)} examples already processed.\")\n",
        "else:\n",
        "    answers_df = pd.DataFrame()\n",
        "    processed_indices = set()\n",
        "\n",
        "# Loop with checkpointing\n",
        "for i, row in annotations_df.iterrows():\n",
        "    key = f\"{row['clip_uid']}::{row['annotation_uid']}\"\n",
        "    '''\n",
        "    if key in processed_keys:\n",
        "        continue  # ‚úÖ Skip already processed\n",
        "        '''\n",
        "\n",
        "    result = process_row_with_gemini(row)\n",
        "    if result:\n",
        "        answers.append(result)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if len(answers) % CHECKPOINT_EVERY == 0:\n",
        "        answers_df = pd.DataFrame(answers)\n",
        "        answers_df.to_csv(CHECKPOINT_PATH, index=False)\n",
        "        print(f\"üíæ Checkpoint saved with {len(answers)} entries.\")\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "# Final save\n",
        "answers_df = pd.DataFrame(answers)\n",
        "answers_df.to_csv(CHECKPOINT_PATH, index=False)\n",
        "print(\"‚úÖ Final save completed. Total entries:\", len(answers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHdkeT7wXUcC",
        "outputId": "e6a15c7c-d09f-462c-834c-df559cc17901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being used in the kitchen.\n",
            "Black cable from monitor.\n",
            "Yes.\n",
            "On the kitchen counter.\n",
            "Bathroom doorway.\n",
            "Hanging on the door.\n",
            "Next to the workbench.\n",
            "White adhesive foam strip.\n",
            "Zero cans on the window.\n",
            "On the stovetop.\n",
            "On the workbench.\n",
            "On the wooden board.\n",
            "On the table.\n",
            "Kitchen sink.\n",
            "On the gray couch.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/17_679cfee6-7da1-4701-b75a-9e34abb9400a_e3015a5a-3e3e-47f5-a6b9-b77d3648621e.mp4.\n",
            "‚ö†Ô∏è Skipping idx 17 due to invalid image(s)\n",
            "Folded in hand.\n",
            "Inside the toolbox drawer.\n",
            "Kitchen cabinet.\n",
            "On the rug.\n",
            "Bottom right, wooden cabinet.\n",
            "üíæ Checkpoint saved with 20 entries.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/26_3341648c-88b4-433a-87ac-1fcc9619a4dc_8e2fed00-ea98-4cc4-b8e7-faecffe28d90.mp4.\n",
            "‚ö†Ô∏è Skipping idx 26 due to invalid image(s)\n",
            "üíæ Checkpoint saved with 20 entries.\n",
            "Nobody.  No interaction shown.\n",
            "One rope.\n",
            "A person in a black hoodie.\n",
            "Outside the window.\n",
            "On the kitchen counter.\n",
            "Shop vacuum.\n",
            "On a balcony.\n",
            "No one.\n",
            "Standing in the hallway.\n",
            "Held in the right hand.\n",
            "Two\n",
            "On the black table.\n",
            "White with dark trim.\n",
            "On the white table.\n",
            "On the tiled floor.\n",
            "On the person's lap.\n",
            "In hand and on brush.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/49_b704e90e-d433-4b13-9f78-f2194c5f3f57_9da625ed-2321-4823-81c8-6679d6468d18.mp4.\n",
            "‚ö†Ô∏è Skipping idx 49 due to invalid image(s)\n",
            "Zero.\n",
            "One carrot.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/55_71e0d3e7-4706-4eb3-9a7a-85d01f46259f_0d80d25e-f3b1-44ec-8ca4-d19bb76a29c9.mp4.\n",
            "‚ö†Ô∏è Skipping idx 55 due to invalid image(s)\n",
            "On the ground.\n",
            "üíæ Checkpoint saved with 40 entries.\n",
            "On the floor.\n",
            "Hands and orange basin.\n",
            "Inside the refrigerator.\n",
            "In the kitchen sink.\n",
            "Kitchen wall, by doorway.\n",
            "On the ground.\n",
            "On the dark surface.\n",
            "Paper towel and plate.\n",
            "Inside the kitchen drawer.\n",
            "Two.\n",
            "Two trays.\n",
            "Six scoops of flour.\n",
            "Dough on a tray.\n",
            "On the metal table.\n",
            "On the metal counter.\n",
            "Into the proofing cabinet.\n",
            "Inside the oven.\n",
            "On a long metal rod.\n",
            "Embedded in the wall.\n",
            "Inside the refrigerator.\n",
            "üíæ Checkpoint saved with 60 entries.\n",
            "No one is visible.\n",
            "On the grass.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/89_f562f1ab-0091-45a0-9e66-4de21d820675_73427cb5-30fd-4118-b01d-e3e15576c944.mp4.\n",
            "‚ö†Ô∏è Skipping idx 89 due to invalid image(s)\n",
            "Long, thin screwdriver.\n",
            "Nobody.\n",
            "Top shelf of closet.\n",
            "Pasta or fries.\n",
            "Nobody else is visible.\n",
            "On the cinder blocks.\n",
            "Not visible in the summary.\n",
            "Inside the black storage bin.\n",
            "Teal or blue-green.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/105_780f7a41-3573-4dd6-8658-d1c8c02ef0ba_19ba979b-ff30-4a79-afe5-b14139c85cc2.mp4.\n",
            "‚ö†Ô∏è Skipping idx 105 due to invalid image(s)\n",
            "Store freezer shelf.\n",
            "White reusable shopping bag.\n",
            "In the kitchen sink.\n",
            "Inside the toolbox drawer.\n",
            "Yes.\n",
            "Held by a person.\n",
            "Held, then worn by woman.\n",
            "Two\n",
            "Near store entrance.\n",
            "üíæ Checkpoint saved with 80 entries.\n",
            "Inside the glass-walled room.\n",
            "On a white cutting board.\n",
            "On the metal counter.\n",
            "Blue and red.\n",
            "Six\n",
            "Inside the refrigerator.\n",
            "Not visible in the video.\n",
            "From lower cabinet.\n",
            "On a store shelf.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/132_6d282ccf-931e-4ee3-a57e-f12447af2f2d_77686398-7e5a-47c2-a2f5-426d43177952.mp4.\n",
            "‚ö†Ô∏è Skipping idx 132 due to invalid image(s)\n",
            "On the floor.\n",
            "No.\n",
            "On the wooden deck.\n",
            "Top drawer, under the stove.\n",
            "White baseboard trim pieces.\n",
            "Near the red shopping baskets.\n",
            "Repair shop floor.\n",
            "Green handled breaker bar.\n",
            "Black, flexible, slotted spatula.\n",
            "Held in the person's hands.\n",
            "Red box by the window.\n",
            "üíæ Checkpoint saved with 100 entries.\n",
            "Nine\n",
            "Gray couch cushion.\n",
            "On the wooden plank.\n",
            "A can of Coca-Cola.\n",
            "Held in hand, kitchen counter.\n",
            "Inside the mower deck assembly.\n",
            "One\n",
            "Cashier with glasses.\n",
            "Two\n",
            "Three\n",
            "On roof rack.\n",
            "Cordless impact wrench.\n",
            "Coffee grounds and a filter.\n",
            "On the metal table.\n",
            "On display table.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/166_fc4bfef7-e079-4783-92e1-b768cfac8125_6f2fcdde-43ad-46f9-8778-40659a86218d.mp4.\n",
            "‚ö†Ô∏è Skipping idx 166 due to invalid image(s)\n",
            "No.\n",
            "Thirty-six.\n",
            "Eggs into carrot mixture.\n",
            "On the wooden bench.\n",
            "Nothing went in the oven.\n",
            "üíæ Checkpoint saved with 120 entries.\n",
            "Metal ruler.\n",
            "Hair from a hairbrush.\n",
            "Two\n",
            "Utensil drawer.\n",
            "In the kitchen sink.\n",
            "Not visible in video.\n",
            "A person in a dark dress.\n",
            "On the kitchen counter.\n",
            "White with blue design.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/186_ed0a0e94-c79b-462d-a64b-238f26fd6fc6_312d4960-1288-4011-9896-05df5aee6c4c.mp4.\n",
            "‚ö†Ô∏è Skipping idx 186 due to invalid image(s)\n",
            "Vacuum cleaner plug.\n",
            "Inside a cupboard.\n",
            "Orange wheelbarrow.\n",
            "On the ground.\n",
            "On the floor.\n",
            "On the white square.\n",
            "Inside the white cabinet.\n",
            "Upright, beneath the table.\n",
            "Not visible in the video.\n",
            "Cardboard and plastic packaging.\n",
            "Bottom drawer, rolling cabinet.\n",
            "üíæ Checkpoint saved with 140 entries.\n",
            "Dark red, possibly maroon.\n",
            "Cutting board next to sink.\n",
            "A large kitchen knife.\n",
            "Three\n",
            "A mixing drill and material.\n",
            "Inside a black bucket.\n",
            "Refrigerator timer.\n",
            "Next to the sink.\n",
            "Near building FL-4310-C.\n",
            "On the road.\n",
            "On the metal table.\n",
            "On the white coffee table.\n",
            "Large stainless steel knife.\n",
            "Next to the sink.\n",
            "A light brown wood block.\n",
            "Six.\n",
            "Inside a black GoPro case.\n",
            "Two\n",
            "White cutting board.\n",
            "Inside the toolbox drawer.\n",
            "üíæ Checkpoint saved with 160 entries.\n",
            "Not visible in the video.\n",
            "On a wooden shelf.\n",
            "No.\n",
            "Gray and orange.\n",
            "Bag of food.\n",
            "Black bungee cord.\n",
            "Inside by the stairs.\n",
            "On the counter, right of sink.\n",
            "In your hands.\n",
            "No.\n",
            "Nine pieces of wood.\n",
            "Near red barrel and shelf.\n",
            "On the floor.\n",
            "One\n",
            "On the green bed.\n",
            "On the patterned surface.\n",
            "Dark red or maroon.\n",
            "Dark red or maroon.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/40_f800514a-5fb0-4620-beb3-69d6c73ddb3f_b308eab3-64e5-451e-8c89-1713bd30b624.mp4.\n",
            "‚ö†Ô∏è Skipping idx 40 due to invalid image(s)\n",
            "In hand, then kitchen counter.\n",
            "Yes.\n",
            "üíæ Checkpoint saved with 180 entries.\n",
            "Tent stake.\n",
            "Inside the refrigerator.\n",
            "Two plates.\n",
            "Dark object, possibly food.\n",
            "Near desert canyon campsite.\n",
            "Flattened pieces of dough.\n",
            "Four\n",
            "‚úÖ Final save completed. Total entries: 187\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}