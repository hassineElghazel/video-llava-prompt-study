{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassineElghazel/video-llava-prompt-study/blob/main/Extension/gemini_1_5flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgcmgtdE8NX"
      },
      "source": [
        "##General Setup\n",
        "\n",
        "Imports and persistant storage in google drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIRFqbWM2x5T",
        "outputId": "3cac8372-f32f-4c91-c9b8-61a8a3407682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71f5JJI5YUaH"
      },
      "outputs": [],
      "source": [
        "# --- standard library ---\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "from io import BytesIO\n",
        "\n",
        "# --- third-party ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import av\n",
        "import torch\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from google.api_core.exceptions import ResourceExhausted, BadRequest, InternalServerError\n",
        "\n",
        "# --- Colab-specific ---\n",
        "from google.colab import drive, userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM6xE9aAE2AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e30f50b-5cb0-467d-d47a-48936aca5702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F5gnv8WFCzn"
      },
      "outputs": [],
      "source": [
        "CUT_VIDEO_OUT = \"/content/drive/MyDrive/model_2.0/model/cut_videos_6tol\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "collapsed": true,
        "id": "T-sMg9_4THT0",
        "outputId": "0221b27b-6625-428b-a55b-5973cb008457"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   idx                        annotation_uid  \\\n",
              "0    0  93891b44-c00f-43d9-885b-92fdce39128c   \n",
              "1    1  00bb3571-8b35-40ba-8163-a1a0fd20b886   \n",
              "2    2  d8fb0c3d-39b1-4bb2-b800-fc56f12ab120   \n",
              "3    3  e8b1352a-6e0c-4f86-8a52-afe93743abb6   \n",
              "4    4  2bc5aa4c-3114-497e-9d13-e229277fe11b   \n",
              "\n",
              "                               clip_uid  \\\n",
              "0  75d3fc52-3776-47d4-b7fd-8074d30b06d1   \n",
              "1  60e7e14d-cbed-46d1-924d-6ce451ea7d7c   \n",
              "2  e1c79556-e8af-4e26-bc4c-633100277239   \n",
              "3  efc190a8-45de-4ce5-b480-b722403bcec1   \n",
              "4  cc6270fd-3c0d-4dda-bcb4-52cefc0224d7   \n",
              "\n",
              "                                        sentence predicted_times  \\\n",
              "0                Where did I put the chopsticks.     (0.0, 3.75)   \n",
              "1                       What cable did I remove?  (78.75, 86.25)   \n",
              "2                  Did I close the refrigerator?     (0.0, 3.75)   \n",
              "3  Where was the scissors before I picked it up?   (93.75, 97.5)   \n",
              "4          In what location did I open the door?     (3.75, 7.5)   \n",
              "\n",
              "            exact_times                             video_uid  \\\n",
              "0        (3.319, 4.619)  413fe086-1745-4573-b75b-e7d26ff72df9   \n",
              "1  (84.32902, 84.71152)  03e90bbc-7d6b-423c-84d9-b5be3eff11c5   \n",
              "2    (0.65536, 2.00097)  4ce119de-0f42-4bd1-b387-9e19643fdddc   \n",
              "3        (87.0, 89.306)  ff6d3d52-dda5-46dd-8515-b9b772933030   \n",
              "4       (6.2829, 8.428)  432cb803-6be5-47bc-8443-6bb5b9051667   \n",
              "\n",
              "        ground truth  \n",
              "0   beside the stove  \n",
              "1          usb drive  \n",
              "2                yes  \n",
              "3  on the plate rack  \n",
              "4    in the bathroom  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2fb535c-5e35-4f82-a255-b50bf071fdad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>annotation_uid</th>\n",
              "      <th>clip_uid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>predicted_times</th>\n",
              "      <th>exact_times</th>\n",
              "      <th>video_uid</th>\n",
              "      <th>ground truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>93891b44-c00f-43d9-885b-92fdce39128c</td>\n",
              "      <td>75d3fc52-3776-47d4-b7fd-8074d30b06d1</td>\n",
              "      <td>Where did I put the chopsticks.</td>\n",
              "      <td>(0.0, 3.75)</td>\n",
              "      <td>(3.319, 4.619)</td>\n",
              "      <td>413fe086-1745-4573-b75b-e7d26ff72df9</td>\n",
              "      <td>beside the stove</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>00bb3571-8b35-40ba-8163-a1a0fd20b886</td>\n",
              "      <td>60e7e14d-cbed-46d1-924d-6ce451ea7d7c</td>\n",
              "      <td>What cable did I remove?</td>\n",
              "      <td>(78.75, 86.25)</td>\n",
              "      <td>(84.32902, 84.71152)</td>\n",
              "      <td>03e90bbc-7d6b-423c-84d9-b5be3eff11c5</td>\n",
              "      <td>usb drive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>d8fb0c3d-39b1-4bb2-b800-fc56f12ab120</td>\n",
              "      <td>e1c79556-e8af-4e26-bc4c-633100277239</td>\n",
              "      <td>Did I close the refrigerator?</td>\n",
              "      <td>(0.0, 3.75)</td>\n",
              "      <td>(0.65536, 2.00097)</td>\n",
              "      <td>4ce119de-0f42-4bd1-b387-9e19643fdddc</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>e8b1352a-6e0c-4f86-8a52-afe93743abb6</td>\n",
              "      <td>efc190a8-45de-4ce5-b480-b722403bcec1</td>\n",
              "      <td>Where was the scissors before I picked it up?</td>\n",
              "      <td>(93.75, 97.5)</td>\n",
              "      <td>(87.0, 89.306)</td>\n",
              "      <td>ff6d3d52-dda5-46dd-8515-b9b772933030</td>\n",
              "      <td>on the plate rack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2bc5aa4c-3114-497e-9d13-e229277fe11b</td>\n",
              "      <td>cc6270fd-3c0d-4dda-bcb4-52cefc0224d7</td>\n",
              "      <td>In what location did I open the door?</td>\n",
              "      <td>(3.75, 7.5)</td>\n",
              "      <td>(6.2829, 8.428)</td>\n",
              "      <td>432cb803-6be5-47bc-8443-6bb5b9051667</td>\n",
              "      <td>in the bathroom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2fb535c-5e35-4f82-a255-b50bf071fdad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2fb535c-5e35-4f82-a255-b50bf071fdad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2fb535c-5e35-4f82-a255-b50bf071fdad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b8b4b47a-10e3-4dc8-927a-ef0a29996252\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8b4b47a-10e3-4dc8-927a-ef0a29996252')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b8b4b47a-10e3-4dc8-927a-ef0a29996252 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations_df",
              "summary": "{\n  \"name\": \"annotations_df\",\n  \"rows\": 197,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 60,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 157,\n        \"samples\": [\n          172,\n          60,\n          181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotation_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 197,\n        \"samples\": [\n          \"b4126ae8-0788-443e-969e-d83dcadd9e56\",\n          \"a6cd5d8f-1de0-4c51-bc5a-1d2c2d7fb086\",\n          \"cac0825d-a367-4213-a934-ded237c49639\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clip_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"9c82fb4e-b385-46a8-b829-45723b17cdba\",\n          \"2c2bda8d-69a3-4a90-9ad6-f6715bc99f39\",\n          \"c96cc4a2-0e82-4c79-b34f-c5b853cd3ab2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 195,\n        \"samples\": [\n          \"where was the ceramic plate before I picked it?\",\n          \"Where was the nylon paper before I picked it?\",\n          \"Where is the dish brush?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_times\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 112,\n        \"samples\": [\n          \"(0.0, 9.375)\",\n          \"(427.0249938964844, 434.51666259765625)\",\n          \"(270.0, 273.75)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact_times\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 194,\n        \"samples\": [\n          \"(53.0, 54.0)\",\n          \"(7.0, 8.0)\",\n          \"(26.29648, 32.507)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 149,\n        \"samples\": [\n          \"28dca1ee-7675-47a2-be89-3fa5979789e1\",\n          \"eb04561c-2ffd-4ea1-aab4-7cadc24db9f9\",\n          \"a7062d19-5af7-42fe-ad8c-2d7442ddbe48\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 172,\n        \"samples\": [\n          \"outside a store\",\n          \"In the baker's hand\",\n          \"Power cable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "annotations_path = f'/content/drive/MyDrive/model_2.0/model/GroundTruth.xlsx'\n",
        "annotations_df = pd.read_excel(annotations_path)\n",
        "\n",
        "annotations_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2uwut71DyCj"
      },
      "source": [
        "Define model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Set and configure API key\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "zAxmw1gCWXpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpWnePWWD-FI"
      },
      "source": [
        "Define model usage functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def build_general_prompt(sentence: str) -> str:\n",
        "    q = sentence.strip().lower()\n",
        "\n",
        "    prompt = (\n",
        "        \"### SYSTEM:\\n\"\n",
        "        \"You are a helpful assistant that answers questions based on visual summaries of video segments.\\n\"\n",
        "        \"Use only the information visible in the video.\\n\"\n",
        "        \"Be accurate and respond in **5 words or fewer**.\\n\\n\"\n",
        "\n",
        "        \"### CONTEXT:\\n\"\n",
        "        \"<video>\\n\"\n",
        "        \"(Visual summary of the video is shown here)\\n\\n\"\n",
        "\n",
        "        f\"### USER:\\nQuestion: {sentence.strip()}\\n\"\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # 🔍 Task Classification Logic\n",
        "    # -------------------------------\n",
        "\n",
        "    # 1. Counting tasks\n",
        "    if re.search(r'\\bhow many\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"This is a counting task. Count only the visible items.\\n\"\n",
        "            \"Return the number.\\n\"\n",
        "        )\n",
        "\n",
        "    # 2. Color detection\n",
        "    elif re.search(r'\\bwhat (color|colour)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the visible color of the mentioned object.\\n\"\n",
        "        )\n",
        "\n",
        "    # 3. Location questions (debiased)\n",
        "    elif re.search(r'\\bwhere (is|was|did|do)\\b|\\bin what location\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the object's visible spatial location based only on the video summary.\\n\"\n",
        "            \"Use only location descriptions directly inferable from the visual context.\\n\"\n",
        "        )\n",
        "\n",
        "    # 4. Action-object identification\n",
        "    elif re.search(r'\\bwhat\\b', q) and re.search(\n",
        "        r'\\b(remove|pick|grab|cut|wash|tie|wipe|press|use|carry|put|insert|drop|place)\\b', q\n",
        "    ):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Identify the object involved in the action based on what is clearly visible in the video.\\n\"\n",
        "        )\n",
        "\n",
        "    # 5. Tool/machine recognition\n",
        "    elif re.search(r'\\bwhat\\b', q) and re.search(r'\\b(tool|machine|device|object|item|equipment)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Determine what object, tool, or machine was used. Base your answer only on visible cues.\\n\"\n",
        "        )\n",
        "\n",
        "    # 6. Human interaction\n",
        "    elif re.search(r'\\bwho did (i|he|she|we|they) (talk|interact)\\b', q):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Name the person involved in the interaction.\\n\"\n",
        "        )\n",
        "\n",
        "    # 7. Yes/no — fallback only\n",
        "    elif (\n",
        "        re.search(r'\\bdid\\b.*\\b(i|we|they|he|she)\\b', q)\n",
        "        and not re.search(r'\\bwhat\\b', q)\n",
        "        and not re.search(r'\\bwho\\b', q)\n",
        "    ):\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"This is a yes/no question. Respond with 'yes' or 'no' based on visible evidence only.\\n\"\n",
        "        )\n",
        "\n",
        "    # 8. Fallback\n",
        "    else:\n",
        "        prompt += (\n",
        "            \"\\n### TASK INSTRUCTION:\\n\"\n",
        "            \"Answer briefly and accurately using only the information visible in the video summary.\\n\"\n",
        "        )\n",
        "\n",
        "    prompt += \"\\nAnswer:\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "X2JKgzdBbYLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_grid_image(frames, grid_size=(2, 4)):\n",
        "    assert len(frames) == 8, \"Exactly 8 frames are expected\"\n",
        "    frame_width, frame_height = frames[0].size\n",
        "    grid_width = grid_size[1] * frame_width\n",
        "    grid_height = grid_size[0] * frame_height\n",
        "\n",
        "    grid_image = Image.new('RGB', (grid_width, grid_height))\n",
        "\n",
        "    for idx, frame in enumerate(frames):\n",
        "        row = idx // grid_size[1]\n",
        "        col = idx % grid_size[1]\n",
        "        grid_image.paste(frame, (col * frame_width, row * frame_height))\n",
        "\n",
        "    return grid_image\n",
        "\n",
        "\n",
        "def read_video_frames(container, indices):\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > indices[-1]:\n",
        "            break\n",
        "        if i in indices:\n",
        "            frames.append(frame.to_ndarray(format='rgb24'))\n",
        "\n",
        "    while len(frames) < 8:\n",
        "        frames.append(frames[-1])\n",
        "\n",
        "    return frames\n",
        "\n",
        "\n",
        "def frame_to_pil(frame):\n",
        "    return Image.fromarray(frame)\n",
        "\n",
        "\n",
        "def prepare_gemini_inputs(row, video_segments_dir):\n",
        "    clip_uid = row['clip_uid']\n",
        "    annotation_uid = row['annotation_uid']\n",
        "    sentence = row['sentence']\n",
        "    idx = int(row['idx'])\n",
        "    video_segment_path = os.path.join(video_segments_dir, f\"{idx}_{clip_uid}_{annotation_uid}.mp4\")\n",
        "\n",
        "    if not os.path.exists(video_segment_path):\n",
        "        print(f\"Video file {video_segment_path} not found.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        container = av.open(video_segment_path)\n",
        "        if not container.streams.video:\n",
        "            print(f\"No video stream in {video_segment_path}.\")\n",
        "            return None, None\n",
        "\n",
        "        total_frames = container.streams.video[0].frames\n",
        "        indices = np.linspace(0, total_frames - 1, num=8, dtype=int)\n",
        "        frames = read_video_frames(container, indices)\n",
        "        pil_frames = [frame_to_pil(f) for f in frames]\n",
        "        grid_image = make_grid_image(pil_frames)  # ✅ create one image from 8\n",
        "\n",
        "        prompt = build_general_prompt(sentence)\n",
        "        return [grid_image], prompt  # ✅ Return one image inside a list\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_segment_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def process_row_with_gemini(row):\n",
        "    images, prompt = prepare_gemini_inputs(row, CUT_VIDEO_OUT)\n",
        "    if images is None or not all(isinstance(img, Image.Image) for img in images):\n",
        "        print(f\"⚠️ Skipping idx {row['idx']} due to invalid image(s)\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            [prompt] + images,\n",
        "            generation_config={\"max_output_tokens\": 3000}\n",
        "        )\n",
        "\n",
        "        if not response.parts:\n",
        "          print(f\"⚠️ No content returned for idx {row['idx']}. Prompt:\\n{prompt}\\n\")\n",
        "          return None\n",
        "\n",
        "        answer = response.text.strip()\n",
        "        print(answer)\n",
        "        times = row['predicted_times'].strip('()').split(\",\")\n",
        "\n",
        "        return {\n",
        "          'idx' : row['idx'],\n",
        "            'annotation_uid' : row['annotation_uid'],\n",
        "            'clip_uid' : row['clip_uid'],\n",
        "            'sentence' : row['sentence'],\n",
        "            'predicted_times' : row['predicted_times'],\n",
        "            'exact_times' : row['exact_times'],\n",
        "            'video_uid' : row['video_uid'],\n",
        "            'ground truth' : row['ground truth'],\n",
        "            'answer': answer.split(\"Answer:\")[-1].strip() if \"Answer:\" in answer else answer.strip(),\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating response for idx {row['idx']}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "aGdlIevR91mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEx-NLPgEH4E"
      },
      "source": [
        "Feed prompts and video into model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/model_2.0/model/gemini_video_qa_ans_1.5_flash_3000.csv\"\n",
        "CHECKPOINT_EVERY = 20\n",
        "\n",
        "answers = []\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    answers_df = pd.read_csv(CHECKPOINT_PATH)\n",
        "    processed_keys = set(answers_df.apply(lambda row: f\"{row['clip_uid']}::{row['annotation_uid']}\", axis=1))\n",
        "    answers = answers_df.to_dict(orient=\"records\")\n",
        "    print(f\"🔁 Resuming from checkpoint: {len(answers)} examples already processed.\")\n",
        "else:\n",
        "    answers_df = pd.DataFrame()\n",
        "    processed_indices = set()\n",
        "\n",
        "# Loop with checkpointing\n",
        "for i, row in annotations_df.iterrows():\n",
        "    key = f\"{row['clip_uid']}::{row['annotation_uid']}\"\n",
        "    '''\n",
        "    if key in processed_keys:\n",
        "        continue  # ✅ Skip already processed\n",
        "        '''\n",
        "\n",
        "    result = process_row_with_gemini(row)\n",
        "    if result:\n",
        "        answers.append(result)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if len(answers) % CHECKPOINT_EVERY == 0:\n",
        "        answers_df = pd.DataFrame(answers)\n",
        "        answers_df.to_csv(CHECKPOINT_PATH, index=False)\n",
        "        print(f\"💾 Checkpoint saved with {len(answers)} entries.\")\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "# Final save\n",
        "answers_df = pd.DataFrame(answers)\n",
        "answers_df.to_csv(CHECKPOINT_PATH, index=False)\n",
        "print(\"✅ Final save completed. Total entries:\", len(answers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHdkeT7wXUcC",
        "outputId": "bc0948b4-ba73-4569-9eae-5a0aca5b010b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Near the stove.\n",
            "Black cable.\n",
            "No\n",
            "On the kitchen counter.\n",
            "Hallway, blue carpet.\n",
            "Near cluttered workbench.\n",
            "Work table, near skull.\n",
            "White tape inside helmet.\n",
            "Zero.\n",
            "On the kitchen counter.\n",
            "On nearby workbench.\n",
            "On wooden surface.\n",
            "On the table, game in progress.\n",
            "In the kitchen sink.\n",
            "On green yoga mat.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/17_679cfee6-7da1-4701-b75a-9e34abb9400a_e3015a5a-3e3e-47f5-a6b9-b77d3648621e.mp4.\n",
            "⚠️ Skipping idx 17 due to invalid image(s)\n",
            "On the kitchen counter.\n",
            "Tool drawer, toolbox.\n",
            "Upper cabinet, right side.\n",
            "On the floor, near rug.\n",
            "Upper cabinet, back shelf.\n",
            "💾 Checkpoint saved with 20 entries.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/26_3341648c-88b4-433a-87ac-1fcc9619a4dc_8e2fed00-ea98-4cc4-b8e7-faecffe28d90.mp4.\n",
            "⚠️ Skipping idx 26 due to invalid image(s)\n",
            "💾 Checkpoint saved with 20 entries.\n",
            "No one visible.\n",
            "Two ropes.\n",
            "Boutique owner.\n",
            "Near the coffee maker.\n",
            "Kitchen counter.\n",
            "Shop vacuum.\n",
            "Outside, balcony, brick wall.\n",
            "No one visible.\n",
            "Near the hallway.\n",
            "On top of wheel hub.\n",
            "Five or six.\n",
            "On the table, near person.\n",
            "White baking tray.\n",
            "On the kitchen table.\n",
            "Bathroom, hallway, and living room.\n",
            "On the table.\n",
            "On person's lap.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/49_b704e90e-d433-4b13-9f78-f2194c5f3f57_9da625ed-2321-4823-81c8-6679d6468d18.mp4.\n",
            "⚠️ Skipping idx 49 due to invalid image(s)\n",
            "Zero\n",
            "Two carrots.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/55_71e0d3e7-4706-4eb3-9a7a-85d01f46259f_0d80d25e-f3b1-44ec-8ca4-d19bb76a29c9.mp4.\n",
            "⚠️ Skipping idx 55 due to invalid image(s)\n",
            "On the ground, near tools.\n",
            "💾 Checkpoint saved with 40 entries.\n",
            "On the workbench.\n",
            "Hands and orange basin.\n",
            "Refrigerator, lower shelf.\n",
            "In the sink.\n",
            "Kitchen counter, near sink.\n",
            "On the ground, near bag.\n",
            "On dirty, dark surface.\n",
            "Paper towels.\n",
            "Drawer, under stove.\n",
            "Two.\n",
            "Two.\n",
            "Four.\n",
            "Bread dough in tray.\n",
            "On the counter.\n",
            "On metal surface, near gloves.\n",
            "On the conveyor belt.\n",
            "Underneath the counter.\n",
            "On the workbench.\n",
            "In the wall.\n",
            "Inside refrigerator, on shelf.\n",
            "💾 Checkpoint saved with 60 entries.\n",
            "No person visible.\n",
            "On the grass, near mat.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/89_f562f1ab-0091-45a0-9e66-4de21d820675_73427cb5-30fd-4118-b01d-e3e15576c944.mp4.\n",
            "⚠️ Skipping idx 89 due to invalid image(s)\n",
            "Long, yellow stick.\n",
            "Myself only.\n",
            "On the bed.\n",
            "Fries on a plate.\n",
            "Nobody else visible.\n",
            "On blocks, near cement.\n",
            "On the ground, near tractor.\n",
            "Inside black storage bin.\n",
            "White and light blue.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/105_780f7a41-3573-4dd6-8658-d1c8c02ef0ba_19ba979b-ff30-4a79-afe5-b14139c85cc2.mp4.\n",
            "⚠️ Skipping idx 105 due to invalid image(s)\n",
            "Frozen food aisle shelf.\n",
            "White plastic bag.\n",
            "In the kitchen sink.\n",
            "Tool box drawer.\n",
            "Yes\n",
            "Person's hand.\n",
            "In hands, then on couch.\n",
            "Zero.\n",
            "Near shopping cart, outside.\n",
            "💾 Checkpoint saved with 80 entries.\n",
            "Hallway, office building.\n",
            "On cutting board, near sink.\n",
            "On the counter.\n",
            "Light blue funnel.\n",
            "At least 7.\n",
            "Inside refrigerator, top shelf.\n",
            "No one visible.\n",
            "Kitchen counter.\n",
            "Store shelf.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/132_6d282ccf-931e-4ee3-a57e-f12447af2f2d_77686398-7e5a-47c2-a2f5-426d43177952.mp4.\n",
            "⚠️ Skipping idx 132 due to invalid image(s)\n",
            "On the floor.\n",
            "No\n",
            "Wooden deck, outdoors.\n",
            "Lower kitchen drawer.\n",
            "White wall trim pieces.\n",
            "Near store entrance.\n",
            "Shop floor, upside down.\n",
            "Green handled tool.\n",
            "Black spatula.\n",
            "Kitchen floor.\n",
            "Red box, near window.\n",
            "💾 Checkpoint saved with 100 entries.\n",
            "Two.\n",
            "Couch armrest.\n",
            "On wooden board.\n",
            "Coca-Cola.\n",
            "Near the kitchen sink.\n",
            "On the yellow machine.\n",
            "Eight bags.\n",
            "Multiple people.\n",
            "Four.\n",
            "Three.\n",
            "Not in the car.\n",
            "Impact driver.\n",
            "Coffee grounds in filter.\n",
            "On the table.\n",
            "Near gray fuzzy items.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/166_fc4bfef7-e079-4783-92e1-b768cfac8125_6f2fcdde-43ad-46f9-8778-40659a86218d.mp4.\n",
            "⚠️ Skipping idx 166 due to invalid image(s)\n",
            "No\n",
            "Approximately twenty-four eggs.\n",
            "Egg mixture.\n",
            "Near wooden slats.\n",
            "Bacon in a pan.\n",
            "💾 Checkpoint saved with 120 entries.\n",
            "Dropped ruler on floor.\n",
            "Hair in dustbin.\n",
            "Eight.\n",
            "Kitchen drawer.\n",
            "In the kitchen sink.\n",
            "No tea towel visible.\n",
            "Another person.\n",
            "On kitchen counter.\n",
            "White with blue design.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/186_ed0a0e94-c79b-462d-a64b-238f26fd6fc6_312d4960-1288-4011-9896-05df5aee6c4c.mp4.\n",
            "⚠️ Skipping idx 186 due to invalid image(s)\n",
            "Vacuum cleaner plug.\n",
            "Kitchen counter, near sink.\n",
            "In orange equipment box.\n",
            "On the ground, near tools.\n",
            "On the floor, in a box.\n",
            "On the workbench.\n",
            "White cabinet, upper shelf.\n",
            "On the workbench.\n",
            "On the ground.\n",
            "A dog and cardboard.\n",
            "Drawer, colorful organizer.\n",
            "💾 Checkpoint saved with 140 entries.\n",
            "Red and white.\n",
            "On a cutting board.\n",
            "A kitchen knife.\n",
            "Three.\n",
            "Mixing concrete with a drill.\n",
            "In a pile of trash.\n",
            "Refrigerator control panel.\n",
            "In the sink, being used.\n",
            "Near building entrance.\n",
            "Road near red rocks.\n",
            "On the counter.\n",
            "On the coffee table.\n",
            "Kitchen knife.\n",
            "Counter, near sink.\n",
            "Wooden block.\n",
            "Eight.\n",
            "Near level and toolbox.\n",
            "Two.\n",
            "White rectangular board.\n",
            "In toolbox drawer.\n",
            "💾 Checkpoint saved with 160 entries.\n",
            "Not visible in video.\n",
            "In a dog cage.\n",
            "No\n",
            "Gray.\n",
            "Food in plastic bag.\n",
            "Black cord.\n",
            "Entryway, near stairs.\n",
            "Near the sink.\n",
            "Near the washing machine.\n",
            "Yes\n",
            "Seven pieces of wood.\n",
            "Garage, near barrels.\n",
            "Floor, near stairs.\n",
            "Two blue tacks.\n",
            "On the bed.\n",
            "On bed, near bag.\n",
            "Dark red/brown.\n",
            "Dark reddish-brown.\n",
            "No video stream in /content/drive/MyDrive/model_2.0/model/cut_videos_6tol/40_f800514a-5fb0-4620-beb3-69d6c73ddb3f_b308eab3-64e5-451e-8c89-1713bd30b624.mp4.\n",
            "⚠️ Skipping idx 40 due to invalid image(s)\n",
            "On the table, in your hands.\n",
            "Yes\n",
            "💾 Checkpoint saved with 180 entries.\n",
            "Tent stake.\n",
            "Inside the refrigerator.\n",
            "Two plates.\n",
            "White cutting board.\n",
            "Desert campsite, near cliffs.\n",
            "Dough in tray.\n",
            "Twenty-four\n",
            "✅ Final save completed. Total entries: 187\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}